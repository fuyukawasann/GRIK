{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","mount_file_id":"1ZGM0vQPKmNL_beyXbvwz6S-y2Lqf65Wk","authorship_tag":"ABX9TyMkdgAZIpYMKyPPXSw7bF1i"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"Wr-QxsM7uljG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ztbD4gjzswmM","executionInfo":{"status":"ok","timestamp":1721052663801,"user_tz":-540,"elapsed":65162,"user":{"displayName":"이한별","userId":"10469278917550586997"}},"outputId":"b5105506-026a-4c8c-ccd1-fb93b07d4800"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting openai-whisper\n","  Downloading openai-whisper-20231117.tar.gz (798 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n","Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.3.0)\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.58.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.25.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.3.0+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.4)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.1.0)\n","Collecting tiktoken (from openai-whisper)\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n","Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.4)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n","Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n","Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.12.2)\n","Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.8)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (24.1)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.41.1)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.31.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper) (3.15.4)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.5.15)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch->openai-whisper)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper)\n","  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.7.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n","Building wheels for collected packages: openai-whisper\n","  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801358 sha256=98633ef322c8aef400f6b08e13d58c1fc4cb432529e82d556cc41ba1f5b4e918\n","  Stored in directory: /root/.cache/pip/wheels/d0/85/e1/9361b4cbea7dd4b7f6702fa4c3afc94877952eeb2b62f45f56\n","Successfully built openai-whisper\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.7.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["whisper"]},"id":"a4dcd3ebc6e84cda8dddbba2d5456337"}},"metadata":{}}],"source":["!pip install openai-whisper librosa"]},{"cell_type":"code","source":["!sudo apt install ffmpeg"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ifs2OT8Cs5C0","executionInfo":{"status":"ok","timestamp":1721052210047,"user_tz":-540,"elapsed":2465,"user":{"displayName":"이한별","userId":"10469278917550586997"}},"outputId":"7147471f-68dc-4f93-c4d6-f05a9ef12791"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"]}]},{"cell_type":"code","source":["import torch\n","import whisper\n","import librosa\n","\n","def transcribe_audio(audio_path, model_name=\"base\", language=None):\n","    # GPU 사용 가능 여부 확인\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    print(f\"Using device: {device}\")\n","\n","    # Whisper 모델 로드 (GPU 사용)\n","    model = whisper.load_model(model_name).to(device)\n","\n","    # MP3 파일 로드\n","    audio, sr = librosa.load(audio_path, sr=16000)\n","\n","    # 오디오 데이터를 GPU로 이동 (필요한 경우)\n","    audio_tensor = torch.from_numpy(audio).to(device)\n","\n","    # Whisper를 사용하여 음성을 텍스트로 변환 (GPU 사용)\n","    with torch.no_grad():\n","        if language:\n","            result = model.transcribe(audio_tensor, language=language)\n","        else:\n","            result = model.transcribe(audio_tensor)\n","\n","    return result\n","\n","# 사용 예시\n","audio_file = input(\"경로를 입력하세요: \")  # MP3 파일 경로를 지정하세요\n","result = transcribe_audio(audio_file, model_name=\"base\")\n","#result = transcribe_audio(audio_file, model_name=\"base\", language=\"ko\")  # 한국어로 설정, 필요에 따라 변경 가능\n","\n","print(result[\"text\"])\n","\n","# 세그먼트별로 결과 출력 (선택사항)\n","for segment in result[\"segments\"]:\n","    print(f\"[{segment['start']:.2f}s -> {segment['end']:.2f}s] {segment['text']}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yWmtIDEqtJg-","executionInfo":{"status":"ok","timestamp":1721052708815,"user_tz":-540,"elapsed":23577,"user":{"displayName":"이한별","userId":"10469278917550586997"}},"outputId":"371fe2b6-2474-4f7a-d443-577d910e0016"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["경로를 입력하세요: /content/drive/MyDrive/IAP/STT/demo.wav\n","Using device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["100%|████████████████████████████████████████| 139M/139M [00:01<00:00, 100MiB/s]\n"]},{"output_type":"stream","name":"stdout","text":[" 문장에서 가장 적절한 따뜻한 고르는 부분입니다. 1번부터 10번까지 총 10무제가 출지되며 각 질문마다 4개의 복기를 들려줍니다.\n","[0.00s -> 5.00s]  문장에서 가장 적절한 따뜻한 고르는 부분입니다.\n","[5.00s -> 11.00s]  1번부터 10번까지 총 10무제가 출지되며 각 질문마다 4개의 복기를 들려줍니다.\n"]}]},{"cell_type":"markdown","source":["## Wave2Vec 2.0 (Facebook AI)"],"metadata":{"id":"vpyPIb4q86s5"}},{"cell_type":"code","source":["from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n","import torch\n","import torchaudio\n","import numpy as np\n","\n","def process_audio_chunk(model, processor, audio_chunk, sampling_rate):\n","    # 16kHz로 리샘플링\n","    resampler = torchaudio.transforms.Resample(sampling_rate, 16000)\n","    audio_chunk = resampler(audio_chunk)\n","\n","    # 모델 입력 준비\n","    inputs = processor(audio_chunk.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n","\n","    # GPU로 데이터 이동\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","    # 음성 인식\n","    with torch.no_grad():\n","        logits = model(**inputs).logits\n","\n","    predicted_ids = torch.argmax(logits, dim=-1)\n","    transcription = processor.batch_decode(predicted_ids)\n","    return transcription\n","\n","# 모델 및 프로세서 로드\n","#model_name = \"facebook/wav2vec2-large-960h\"\n","model_name = \"kresnik/wav2vec2-large-xlsr-korean\"\n","processor = Wav2Vec2Processor.from_pretrained(model_name)\n","model = Wav2Vec2ForCTC.from_pretrained(model_name)\n","\n","# GPU 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# 오디오 파일 로드\n","speech_array, sampling_rate = torchaudio.load('/content/drive/MyDrive/IAP/STT/demo.wav')\n","\n","# 스테레오를 모노로 변환 (필요한 경우)\n","if speech_array.shape[0] > 1:\n","    speech_array = torch.mean(speech_array, dim=0, keepdim=True)\n","\n","# 청크 단위로 오디오 처리\n","chunk_size = 10 * sampling_rate  # 10초 단위로 청크 나누기\n","transcriptions = []\n","\n","for start in range(0, speech_array.shape[1], chunk_size):\n","    end = min(start + chunk_size, speech_array.shape[1])\n","    audio_chunk = speech_array[:, start:end]\n","    transcription = process_audio_chunk(model, processor, audio_chunk, sampling_rate)\n","    transcriptions.extend(transcription)\n","\n","# 전체 텍스트 출력\n","full_transcription = ' '.join(transcriptions)\n","print(full_transcription)\n","\n"],"metadata":{"id":"yEk_UboSuKyo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721560645735,"user_tz":-540,"elapsed":4864,"user":{"displayName":"이한별","userId":"10469278917550586997"}},"outputId":"914aa521-9c4c-46d3-e04e-bd760f1eebfe"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at kresnik/wav2vec2-large-xlsr-korean were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n","- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at kresnik/wav2vec2-large-xlsr-korean and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["문장에서 가장 적절한 다뼈 고르던 부분입니 일본부터 십 억까지 총 열은제가 출제되며 각 증원마받다 네 개 다 기을 들려 줍니다\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"pxjXzZEl-pPu"},"execution_count":null,"outputs":[]}]}